<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Learning the ground-state of a spin model with different Neural Networks available in NetKet &mdash; netket v3.0 documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
      <link rel="stylesheet" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Getting Started" href="../docs/getting_started.html" />
    <link rel="prev" title="Variational Monte Carlo with Neural Networks" href="j1j2.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> netket<img src="../_static/logonav.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="netket3.html">Ground-State Variational Search with NetKet</a></li>
<li class="toctree-l1"><a class="reference internal" href="jax.html">Using JAX as a backend in NetKet - Feature Preview for v3.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="G-CNN_Honeycomb.html">Using a group convolutional neural network to learn the ground-state of a symmetric spin model</a></li>
<li class="toctree-l1"><a class="reference internal" href="G-CNN_Honeycomb.html#G-CNNs-are-generalizations-of-CNNs-to-non-abelian-groups">G-CNNs are generalizations of CNNs to non-abelian groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="G-CNN_Honeycomb.html#Defining-the-Hamiltonian">Defining the Hamiltonian</a></li>
<li class="toctree-l1"><a class="reference internal" href="G-CNN_Honeycomb.html#Defining-the-GCNN">Defining the GCNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="G-CNN_Honeycomb.html#Variational-Monte-Carlo">Variational Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="G-CNN_Honeycomb.html#Checking-with-ED">Checking with ED</a></li>
<li class="toctree-l1"><a class="reference internal" href="G-CNN_Honeycomb.html#Simulating-A-Larger-Lattice">Simulating A Larger Lattice</a></li>
<li class="toctree-l1"><a class="reference internal" href="j1j2.html">Variational Monte Carlo with Neural Networks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Learning the ground-state of a spin model with different Neural Networks available in NetKet</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#0)-Defining-the-Hamiltonian">0) Defining the Hamiltonian</a></li>
<li class="toctree-l2"><a class="reference internal" href="#1.-Exact-Diagonalization-(as-a-testbed)">1. Exact Diagonalization (as a testbed)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#2.-Starting-simple:-the-Jastrow-ansatz">2. Starting simple: the Jastrow ansatz</a></li>
<li class="toctree-l2"><a class="reference internal" href="#3.-Learning-with-a-Restricted-Boltzmann-Machine-(RBM)">3. Learning with a Restricted Boltzmann Machine (RBM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#4.-RBM-again,-this-time-with-lattice-symmetries">4. RBM again, this time with lattice symmetries</a></li>
<li class="toctree-l2"><a class="reference internal" href="#5.-Learning-with-Feed-Forward-Neural-Networks">5. Learning with Feed Forward Neural Networks</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Reference Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../docs/getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/changelog.html">Change Log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/superop.html">The Lindblad Master Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/hilbert.html">The Hilbert module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/operator.html">The Operator module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/varstate.html">The Variational State Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/sr.html">Quantum Geometric Tensor and Stochastic Reconfiguration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/drivers.html">The Drivers API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/sharp-bits.html">üî™ The Sharp Bits üî™</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending NetKet</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../docs/custom_models.html">Defining Custom Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/custom_expect.html">Overriding defaults in NetKet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/custom_preconditioners.html">Defining Custom Preconditioners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/custom_operator.html">Defining Custom Operators</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../docs/contributing.html">Contributing to NetKet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/writing-tests.html">Writing Tests</a></li>
</ul>
<p class="caption"><span class="caption-text">API documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../docs/api-stability.html">API Stability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/api.html">Public API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/api-experimental.html">Experimental API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">netket</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Learning the ground-state of a spin model with different Neural Networks available in NetKet</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/Heisenberg1d.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Learning-the-ground-state-of-a-spin-model-with-different-Neural-Networks-available-in-NetKet">
<h1>Learning the ground-state of a spin model with different Neural Networks available in NetKet<a class="headerlink" href="#Learning-the-ground-state-of-a-spin-model-with-different-Neural-Networks-available-in-NetKet" title="Permalink to this headline">ÔÉÅ</a></h1>
<p>The goal of this tutorial is to review various neural network architectures available in NetKet, and this in order to learn the ground-state of a paradigmatic spin model, namely the spin <span class="math notranslate nohighlight">\(1/2\)</span> Heisenberg antiferromagnetic spin chain.</p>
<p>The Hamiltonian we will consider for this tutorial is the following</p>
<div class="math notranslate nohighlight">
\[H = \sum_{i=1}^{L} \vec{\sigma}_{i} \cdot \vec{\sigma}_{i+1}.\]</div>
<p><span class="math notranslate nohighlight">\(L\)</span> is the length of the chain, and we will use both open and periodic boundary conditions. <span class="math notranslate nohighlight">\(\vec{\sigma}=(\sigma^x,\sigma^y,\sigma^z)\)</span> denotes the vector of Pauli matrices. Please note that there is a factor of <span class="math notranslate nohighlight">\(2\)</span> between Pauli-matrices and spin-1/2 operators (thus a factor of <span class="math notranslate nohighlight">\(4\)</span> in <span class="math notranslate nohighlight">\(H\)</span>).</p>
<p>We will consider in this tutorial 5 possible ways of determining the ground-state of this model.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0. Defining the Hamiltonian
1. Exact Diagonalization (as a testbed)
2. Starting simple: the Jastrow ansatz
3. Learning with a Restricted Boltzmann Machine (RBM)
4. RBM again, this time with lattice symmetries
5. Learning with Feed Forward Neural Networks
</pre></div>
</div>
<p>Estimated time for this tutorial: 20 minutes.</p>
<p>First things first, let‚Äôs import the essentials</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># Import netket library
import netket as nk

# Import Json, this will be needed to examine log files
import json

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
import time
</pre></div>
</div>
</div>
<div class="section" id="0)-Defining-the-Hamiltonian">
<h2>0) Defining the Hamiltonian<a class="headerlink" href="#0)-Defining-the-Hamiltonian" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>NetKet covers quite a few standard Hamiltonians and lattices, so let‚Äôs use this to quickly define the antiferromagnetic Heisenberg chain. For the moment we assume <span class="math notranslate nohighlight">\(L=22\)</span> and simply define a chain lattice in this way (using periodic boundary conditions for now).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># Define a 1d chain
L = 22
g = nk.graph.Hypercube(length=L, n_dim=1, pbc=True)
</pre></div>
</div>
</div>
<p>Next, we need to define the Hilbert space on this graph. We have here spin-half degrees of freedom, and as we know that the ground-state sits in the zero magnetization sector, we can already impose this as a constraint in the Hilbert space. This is not mandatory, but will nicely speeds things up in the following.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># Define the Hilbert space based on this graph
# We impose to have a fixed total magnetization of zero
hi = nk.hilbert.Spin(s=0.5, total_sz=0, N=g.n_nodes)
</pre></div>
</div>
</div>
<p>The final element of the triptych is of course the Hamiltonian acting in this Hilbert space, which in our case in already defined in NetKet. Note that the NetKet Hamiltonian uses Pauli Matrices (if you prefer to work with spin-<span class="math notranslate nohighlight">\(1/2\)</span> operators, it‚Äôs pretty trivial to define your own custom Hamiltonian, as covered in another tutorial)</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># calling the Heisenberg Hamiltonian
ha = nk.operator.Heisenberg(hilbert=hi, graph=g)
</pre></div>
</div>
</div>
</div>
<div class="section" id="1.-Exact-Diagonalization-(as-a-testbed)">
<h2>1. Exact Diagonalization (as a testbed)<a class="headerlink" href="#1.-Exact-Diagonalization-(as-a-testbed)" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Just as a matter of future comparison, let‚Äôs compute the exact ground-state energy (since this is still possible for <span class="math notranslate nohighlight">\(L=22\)</span> using brute-force exact diagonalization). NetKet provides wrappers to the Lanczos algorithm which we now use:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># compute the ground-state energy (here we only need the lowest energy, and do not need the eigenstate)
evals = nk.exact.lanczos_ed(ha, compute_eigenvectors=False)
exact_gs_energy = evals[0]
print(&#39;The exact ground-state energy is E0=&#39;,exact_gs_energy)

# Just in case you can&#39;t do this calculation, here is the result
# exact_gs_energy = -39.14752260706246
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The exact ground-state energy is E0= -39.14752260706275
</pre></div></div>
</div>
</div>
<div class="section" id="2.-Starting-simple:-the-Jastrow-ansatz">
<h2>2. Starting simple: the Jastrow ansatz<a class="headerlink" href="#2.-Starting-simple:-the-Jastrow-ansatz" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Let‚Äôs start with a simple ansatz for the ground-state: the Jastrow Ansatz.</p>
<p>$ <span class="math">\log</span><span class="math">\psi`(:nbsphinx-math:</span>sigma`) = <span class="math">\sum</span>_i a_i <span class="math">\sigma</span><em>i + :nbsphinx-math:`sum`</em>{i,j} <span class="math">\sigma</span><em>i J</em>{i,j} <span class="math">\sigma</span>_j $</p>
<p>To show how it‚Äôs done, we write this simple ansatz as a flax.linen module. Flax.linen is wrapped inside of netket and it‚Äôs made to better work with complex numbers, so instead of using flax.linen you should use nk.nn.</p>
<p>In the example below, we write a function <code class="docutils literal notranslate"><span class="pre">single_evaluate</span></code> that evaluates a single bit-string, and then we use jax.vmap to make sure that it works for batches of inputs</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import netket.nn as nknn
import flax.linen as nn
import jax.numpy as jnp
import jax

class Jastrow(nknn.Module):
    @nknn.compact
    def __call__(self, x):
        x = jnp.atleast_2d(x)
        return jax.vmap(self.single_evaluate, in_axes=(0))(x)

    def single_evaluate(self, x):
        v_bias = self.param(
            &quot;visible_bias&quot;, nn.initializers.normal(), (x.shape[-1],), complex
        )

        J = self.param(
            &quot;kernel&quot;, nn.initializers.normal(), (x.shape[-1],x.shape[-1]), complex
        )

        return x.T@J@x + jnp.dot(x, v_bias)

ma = Jastrow()
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># Build the sampler
sa = nk.sampler.MetropolisExchange(hilbert=hi,graph=g)

# Optimizer
op = nk.optimizer.Sgd(learning_rate=0.1)

# Stochastic Reconfiguration
sr = nk.optimizer.SR(diag_shift=0.1)

# The variational state
vs = nk.vqs.MCState(sa, ma, n_samples=1000)

# The ground-state optimization loop
gs = nk.VMC(
    hamiltonian=ha,
    optimizer=op,
    preconditioner=sr,
    variational_state=vs)

start = time.time()
gs.run(300, out=&#39;Jastrow&#39;)
end = time.time()

print(&#39;### Jastrow calculation&#39;)
print(&#39;Has&#39;,nk.jax.tree_size(vs.parameters),&#39;parameters&#39;)
print(&#39;The Jastrow calculation took&#39;,end-start,&#39;seconds&#39;)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:21&lt;00:00, 14.21it/s, Energy=-39.178-0.000j ¬± 0.016 [œÉ¬≤=0.271, RÃÇ=1.0052]]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
### Jastrow calculation
Has 506 parameters
The Jastrow calculation took 26.639398336410522 seconds
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># import the data from log file
data_Jastrow=json.load(open(&quot;Jastrow.log&quot;))

iters_Jastrow = data_Jastrow[&quot;Energy&quot;][&quot;iters&quot;]
energy_Jastrow = data_Jastrow[&quot;Energy&quot;][&quot;Mean&quot;][&quot;real&quot;]

fig, ax1 = plt.subplots()
ax1.plot(iters_Jastrow, energy_Jastrow, color=&#39;C8&#39;, label=&#39;Energy (Jastrow)&#39;)
ax1.set_ylabel(&#39;Energy&#39;)
ax1.set_xlabel(&#39;Iteration&#39;)
plt.axis([0,iters_Jastrow[-1],exact_gs_energy-0.1,exact_gs_energy+0.4])
plt.axhline(y=exact_gs_energy, xmin=0,
                xmax=iters_Jastrow[-1], linewidth=2, color=&#39;k&#39;, label=&#39;Exact&#39;)
ax1.legend()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorials_Heisenberg1d_14_0.png" src="../_images/tutorials_Heisenberg1d_14_0.png" />
</div>
</div>
<p>Well that‚Äôs not too bad for a simple ansatz. But we can do better, can‚Äôt we?</p>
</div>
<div class="section" id="3.-Learning-with-a-Restricted-Boltzmann-Machine-(RBM)">
<h2>3. Learning with a Restricted Boltzmann Machine (RBM)<a class="headerlink" href="#3.-Learning-with-a-Restricted-Boltzmann-Machine-(RBM)" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>We will now consider another celebrated ansatz, the Restricted Boltzmann Machine (RBM). It simply consists of two layers: a visible one representing the <span class="math notranslate nohighlight">\(L\)</span> spin 1/2 degrees of freedom, and an hidden one which contains a different number <span class="math notranslate nohighlight">\(M\)</span> of hidden units. There are connections between all visible and hidden nodes. The ansatz is the <a class="reference external" href="https://www.netket.org/docs/machine_RbmSpin/">following</a></p>
<p><span class="math notranslate nohighlight">\(\Psi_{\rm RBM} (\sigma_1^z,\sigma_2^z, ..., \sigma_L^z) = \exp ( \sum_{i=1}^L a_i \sigma_i^z ) \prod_{i=1}^M \cosh (b_i + \sum_j W_{ij} \sigma^z_j)\)</span></p>
<p><span class="math notranslate nohighlight">\(a_i\)</span> (resp. <span class="math notranslate nohighlight">\(b_i\)</span>) are the visible (resp. hidden) bias. Together with the weights <span class="math notranslate nohighlight">\(W_{ij}\)</span>, they are variational parameters that we (or rather NetKet) will optimize to minimize the energy. Netket gives you the control on the important parameters in this ansatz, such as <span class="math notranslate nohighlight">\(M\)</span> and the fact that you want to use or not the biases. The full explanation is <a class="reference external" href="https://www.netket.org/docs/machine_RbmSpin/">here</a>.</p>
<p>More conveniently (especially if you want to try another <span class="math notranslate nohighlight">\(L\)</span> in this tutorial), let‚Äôs define the hidden unit density <span class="math notranslate nohighlight">\(\alpha = M / L\)</span>, and invoke the RBM ansatz in NetKet with as many hidden as visible units.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># RBM ansatz with alpha=1
ma = nk.models.RBM(alpha=1)
</pre></div>
</div>
</div>
<p>And let us use the same sampler (Metropolis exchange) with some different random initial parameters, optimizer (stochastic gradient), and variational method (stochastic reconfiguration) as for the Jastrow ansatz, and let‚Äôs run things!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># Build the sampler
sa = nk.sampler.MetropolisExchange(hilbert=hi,graph=g)

# Optimizer
op = nk.optimizer.Sgd(learning_rate=0.05)

# Stochastic Reconfiguration
sr = nk.optimizer.SR(diag_shift=0.1)

# The variational state
vs = nk.vqs.MCState(sa, ma, n_samples=1000)

# The ground-state optimization loop
gs = nk.VMC(
    hamiltonian=ha,
    optimizer=op,
    preconditioner=sr,
    variational_state=vs)

start = time.time()
gs.run(out=&#39;RBM&#39;, n_iter=600)
end = time.time()

print(&#39;### RBM calculation&#39;)
print(&#39;Has&#39;,vs.n_parameters,&#39;parameters&#39;)
print(&#39;The RBM calculation took&#39;,end-start,&#39;seconds&#39;)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [00:41&lt;00:00, 14.49it/s, Energy=-39.118 ¬± 0.014 [œÉ¬≤=0.200, RÃÇ=0.9978]]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
### RBM calculation
Has 528 parameters
The RBM calculation took 46.16197657585144 seconds
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># import the data from log file
data=json.load(open(&quot;RBM.log&quot;))

# Extract the relevant information
iters_RBM = data[&quot;Energy&quot;][&quot;iters&quot;]
energy_RBM = data[&quot;Energy&quot;][&quot;Mean&quot;]

fig, ax1 = plt.subplots()
ax1.plot(iters_Jastrow, energy_Jastrow, color=&#39;C8&#39;, label=&#39;Energy (Jastrow)&#39;)
ax1.plot(iters_RBM, energy_RBM, color=&#39;red&#39;, label=&#39;Energy (RBM)&#39;)
ax1.set_ylabel(&#39;Energy&#39;)
ax1.set_xlabel(&#39;Iteration&#39;)
plt.axis([0,iters_RBM[-1],exact_gs_energy-0.03,exact_gs_energy+0.2])
plt.axhline(y=exact_gs_energy, xmin=0,
                xmax=iters_RBM[-1], linewidth=2, color=&#39;k&#39;, label=&#39;Exact&#39;)
ax1.legend()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorials_Heisenberg1d_20_0.png" src="../_images/tutorials_Heisenberg1d_20_0.png" />
</div>
</div>
<p>Note that this plot zooms closer to the exact ground-state energy. With 600 iterations, we start to see convergence and reach the ground-state energy within about one per thousand, this is already nice! But we are not totally there yet, and in particular the simpler (less parameters) Jastrow wave-function seems to perform better for this example. How can we improve things? As an exercice, try to increase the number of hidden units and/or the number of iterations. What is happening? You can also
check out the influence of the learning rate.</p>
<p>By playing with these parameters, you have hopefully arrived at an improved result, but likely at an increased CPU time cost. Let‚Äôs do things differently, and take to our advantage the symmetries of the Hamiltonian in the neural network construction.</p>
</div>
<div class="section" id="4.-RBM-again,-this-time-with-lattice-symmetries">
<h2>4. RBM again, this time with lattice symmetries<a class="headerlink" href="#4.-RBM-again,-this-time-with-lattice-symmetries" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Let‚Äôs define a similar RBM machine, which takes into account that the model has translational symmetries. All sites are equivalent and thus many of the wave-functions coefficients are related by symmetry. We use the same exact hyperparameters as in the previous RBM calculation (<span class="math notranslate nohighlight">\(\alpha=1\)</span>, same learning rate, and number of samples and iterations in the Variational Monte Carlo) and run now a symmetric RBM.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>## Symmetric RBM Spin Machine
ma = nk.models.RBMSymm(symmetries=g.translation_group(), alpha=1)

# Metropolis Exchange Sampling
# Notice that this sampler exchanges two neighboring sites
# thus preservers the total magnetization
sa = nk.sampler.MetropolisExchange(hi, graph=g)

# Optimizer
op = nk.optimizer.Sgd(learning_rate=0.05)

# Stochastic Reconfiguration
sr = nk.optimizer.SR(diag_shift=0.1)

# The variational state
vs = nk.vqs.MCState(sa, ma, n_samples=1000)

# The ground-state optimization loop
gs = nk.VMC(
    hamiltonian=ha,
    optimizer=op,
    preconditioner=sr,
    variational_state=vs)

start = time.time()
gs.run(out=&#39;RBMSymmetric&#39;, n_iter=300)
end = time.time()

print(&#39;### Symmetric RBM calculation&#39;)
print(&#39;Has&#39;,vs.n_parameters,&#39;parameters&#39;)
print(&#39;The Symmetric RBM calculation took&#39;,end-start,&#39;seconds&#39;)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:17&lt;00:00, 16.70it/s, Energy=-39.125 ¬± 0.011 [œÉ¬≤=0.116, RÃÇ=1.0049]]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
### Symmetric RBM calculation
Has 24 parameters
The Symmetric RBM calculation took 22.50969409942627 seconds
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<p>The simulation was much faster, wasn‚Äôt it? There were of course much less parameters to optimize. Now let‚Äôs extract the results and plot them using a zoomed scale, and together with the previous results with the RBM.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>## import the data from log file
data=json.load(open(&quot;RBMSymmetric.log&quot;))

# Extract the relevant information
iters_symRBM = data[&quot;Energy&quot;][&quot;iters&quot;]
energy_symRBM = data[&quot;Energy&quot;][&quot;Mean&quot;]

fig, ax1 = plt.subplots()
ax1.plot(iters_Jastrow, energy_Jastrow, color=&#39;C8&#39;, label=&#39;Energy (Jastrow)&#39;)
ax1.plot(iters_RBM, energy_RBM, color=&#39;red&#39;, label=&#39;Energy (RBM)&#39;)
ax1.plot(iters_symRBM, energy_symRBM, color=&#39;blue&#39;, label=&#39;Energy (Symmetric RBM)&#39;)

ax1.set_ylabel(&#39;Energy&#39;)
ax1.set_xlabel(&#39;Iteration&#39;)
if exact_gs_energy:
    plt.axis([0,iters_symRBM[-1],exact_gs_energy-0.06,exact_gs_energy+0.12])
    plt.axhline(y=exact_gs_energy, xmin=0,
                    xmax=iters_RBM[-1], linewidth=2, color=&#39;k&#39;, label=&#39;Exact&#39;)
ax1.legend()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorials_Heisenberg1d_25_0.png" src="../_images/tutorials_Heisenberg1d_25_0.png" />
</div>
</div>
<p>Not only the simulation was faster in terms of CPU time, but we are now reaching the ground-state in a much lower number of iterations! Imposing symmetries greatly helps, and NetKet allows to do this. Note that there is also a symmetric version of the Jastrow ansatz that we tested <a class="reference external" href="#2.-Starting-simple:-the-Jastrow-ansatz">earlier</a> in NetKet, which is called <code class="docutils literal notranslate"><span class="pre">JastrowSymm</span></code>. As an exercice, check it out. What you will find is that while it converges slightly faster in terms of iterations with
respect to the non-symmetric Jastrow, it does not improve the estimate of the ground-state energy. We actually see that the symmetric RBM sets the standard very high.</p>
</div>
<div class="section" id="5.-Learning-with-Feed-Forward-Neural-Networks">
<h2>5. Learning with Feed Forward Neural Networks<a class="headerlink" href="#5.-Learning-with-Feed-Forward-Neural-Networks" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Now let‚Äôs try a more complex network, namely a Feed Forward Neural Network (FFNN). There you will have more freedom to construct your own specific architecture. We‚Äôll try two different FFNN in this tutorial.</p>
<p>The first one is a simple structure: the first layer takes as input L-dimensional input, applies a bias and outputs two times more data, just followed by a <code class="docutils literal notranslate"><span class="pre">Lncosh</span></code> activation layer. The final layer <code class="docutils literal notranslate"><span class="pre">SumOuput</span></code> is needed to obtain a single number for the wave-function coefficient associated to the input basis state.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>class Model(nk.nn.Module):
    @nk.nn.compact
    def __call__(self, x):
        x = nk.nn.Dense(features=2*x.shape[-1], dtype=np.complex128, kernel_init=nk.nn.initializers.normal(stddev=0.1), bias_init=nk.nn.initializers.normal(stddev=0.1))(x)
        x = nk.nn.activation.log_cosh(x)
        return jax.numpy.sum(x, axis=-1)

ffnn = Model()

sa = nk.sampler.MetropolisExchange(hi, graph=g)

# The variational state
vs = nk.vqs.MCState(sa, ffnn, n_samples=1000)

opt = nk.optimizer.Sgd(learning_rate=0.05)

# Stochastic Reconfiguration
sr = nk.optimizer.SR(diag_shift=0.1)

# The ground-state optimization loop
gs = nk.VMC(
    hamiltonian=ha,
    optimizer=op,
    preconditioner=sr,
    variational_state=vs)


start = time.time()
gs.run(out=&#39;FF&#39;, n_iter=300)
end = time.time()

print(&#39;### Feed Forward calculation&#39;)
print(&#39;Has&#39;,vs.n_parameters,&#39;parameters&#39;)
print(&#39;The Feed Forward calculation took&#39;,end-start,&#39;seconds&#39;)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [01:48&lt;00:00,  2.78it/s, Energy=-39.109-0.013j ¬± 0.018 [œÉ¬≤=0.322, RÃÇ=1.0027]]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
### Feed Forward calculation
Has 1012 parameters
The Feed Forward calculation took 113.18418622016907 seconds
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># import the data from log file
data=json.load(open(&quot;FF.log&quot;))

# Extract the relevant information
iters_FF = data[&quot;Energy&quot;][&quot;iters&quot;]
energy_FF = data[&quot;Energy&quot;][&quot;Mean&quot;][&quot;real&quot;]

fig, ax1 = plt.subplots()
ax1.plot(iters_Jastrow, energy_Jastrow, color=&#39;C8&#39;,linestyle=&quot;None&quot;, marker=&#39;d&#39;,label=&#39;Energy (Jastrow)&#39;)
ax1.plot(iters_RBM, energy_RBM, color=&#39;red&#39;, marker=&#39;o&#39;,linestyle=&quot;None&quot;,label=&#39;Energy (RBM)&#39;)
ax1.plot(iters_symRBM, energy_symRBM, color=&#39;blue&#39;,linestyle=&quot;None&quot;,marker=&#39;o&#39;,label=&#39;Energy (Symmetric RBM)&#39;)
ax1.plot(iters_FF, energy_FF, color=&#39;orange&#39;, marker=&#39;s&#39;,linestyle=&quot;None&quot;,label=&#39;Energy (Feed Forward, take 1)&#39;)
ax1.legend(bbox_to_anchor=(1.05, 0.3))
ax1.set_ylabel(&#39;Energy&#39;)
ax1.set_xlabel(&#39;Iteration&#39;)
plt.axis([0,iters_FF[-1],exact_gs_energy-0.02,exact_gs_energy+0.1])
plt.axhline(y=exact_gs_energy, xmin=0,
                xmax=iters_RBM[-1], linewidth=2, color=&#39;k&#39;, label=&#39;Exact&#39;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorials_Heisenberg1d_29_0.png" src="../_images/tutorials_Heisenberg1d_29_0.png" />
</div>
</div>
<p>The results are clearly better than a simple (non-symmetrized RBB), and perform slightly better than the Jastrow ansatz. Let us increase the number of layers by adding a fully-connected layer with bias and <code class="docutils literal notranslate"><span class="pre">Lncosh</span></code> activation (with <span class="math notranslate nohighlight">\(2L\)</span> inputs and ouputs) before the final <code class="docutils literal notranslate"><span class="pre">SumOuput</span></code> layer.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>class Model2(nk.nn.Module):
    @nk.nn.compact
    def __call__(self, x):
        x = nk.nn.Dense(features=2*x.shape[-1], dtype=np.complex128, kernel_init=nk.nn.initializers.normal(stddev=0.1), bias_init=nk.nn.initializers.normal(stddev=0.1))(x)
        x = nk.nn.activation.log_cosh(x)
        x = nk.nn.Dense(features=x.shape[-1], dtype=np.complex128, kernel_init=nk.nn.initializers.normal(stddev=0.1), bias_init=nk.nn.initializers.normal(stddev=0.1))(x)
        x = nk.nn.activation.log_cosh(x)
        return jax.numpy.sum(x, axis=-1)

ffnn2 = Model2()

# The variational state
vs = nk.vqs.MCState(sa, ffnn, n_samples=1000)

opt = nk.optimizer.Sgd(learning_rate=0.05)

# Stochastic Reconfiguration
sr = nk.optimizer.SR(diag_shift=0.1)

# The ground-state optimization loop
gs = nk.VMC(
    hamiltonian=ha,
    optimizer=op,
    preconditioner=sr,
    variational_state=vs)


start = time.time()
gs.run(out=&#39;FF2&#39;, n_iter=600)
end = time.time()


print(&#39;### Feed Forward (more layers) calculation&#39;)
print(&#39;Has&#39;,vs.n_parameters,&#39;parameters&#39;)
print(&#39;The Feed Forward (more layers) calculation took&#39;,end-start,&#39;seconds&#39;)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [03:35&lt;00:00,  2.78it/s, Energy=-39.130-0.009j ¬± 0.015 [œÉ¬≤=0.226, RÃÇ=0.9973]]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
### Feed Forward (more layers) calculation
Has 1012 parameters
The Feed Forward (more layers) calculation took 216.8182888031006 seconds
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># import the data from log file
data=json.load(open(&quot;FF2.log&quot;))

# Extract the relevant information
iters_FF_morelayers = data[&quot;Energy&quot;][&quot;iters&quot;]
energy_FF_morelayers = data[&quot;Energy&quot;][&quot;Mean&quot;][&quot;real&quot;]

fig, ax1 = plt.subplots()
#ax1.plot(iters_Jastrow, energy_Jastrow, color=&#39;C8&#39;,linestyle=&quot;None&quot;, marker=&#39;d&#39;,label=&#39;Energy (Jastrow)&#39;)
#ax1.plot(iters_RBM, energy_RBM, color=&#39;red&#39;, label=&#39;Energy (RBM)&#39;)
#ax1.plot(iters_symRBM, energy_symRBM, color=&#39;blue&#39;,linestyle=&quot;None&quot;,marker=&#39;o&#39;,label=&#39;Energy (Symmetric RBM)&#39;)
ax1.plot(iters_FF, energy_FF, color=&#39;orange&#39;, marker=&#39;s&#39;,alpha=0.5,linestyle=&quot;None&quot;,label=&#39;Energy (Feed Forward, take 1)&#39;)
ax1.plot(iters_FF_morelayers, energy_FF_morelayers, color=&#39;green&#39;,marker=&#39;s&#39;,linestyle=&quot;None&quot;, alpha=1,label=&#39;Energy (Feed Forward, more layers)&#39;)
ax1.legend(bbox_to_anchor=(1.05, 0.5))
ax1.set_ylabel(&#39;Energy&#39;)
ax1.set_xlabel(&#39;Iteration&#39;)
plt.axis([0,iters_RBM[-1],exact_gs_energy-0.02,exact_gs_energy+0.06])
plt.axhline(y=exact_gs_energy, xmin=0,
                xmax=iters_RBM[-1], linewidth=2, color=&#39;k&#39;, label=&#39;Exact&#39;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorials_Heisenberg1d_32_0.png" src="../_images/tutorials_Heisenberg1d_32_0.png" />
</div>
</div>
<p>The results are even better, but at the price of an increase in computational time‚Ä¶.</p>
<p>Note that more complex structures, such as Convolutional Neural Networks (CNN), can also be used within Netket. However, for such 1d systems, they do not bring too much compared to the symmetric RBM (as a matter of fact, the symmetric RBM is a special type of a simple CNN. CNNs show their full strength for more complex systems, such as 2d quantum systems. Convolutional Neural Networks will be the topic of another tutorial in NetKet (and we‚Äôll make there the connection with the special case of
the symmetric RBM).</p>
<p>Finally let us conclude that another type of machine, Matrix Product States (MPS), is also available in NetKet. They do perform extremely well for 1d quantum systems. Since however they are a bit different, they will be presented in another tutorial.</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="j1j2.html" class="btn btn-neutral float-left" title="Variational Monte Carlo with Neural Networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../docs/getting_started.html" class="btn btn-neutral float-right" title="Getting Started" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019-2021, The Netket authors - All rights reserved.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 

<script type="text/javascript">
    jQuery(function () {
        SphinxRtdTheme.Navigation.enable(true);
      })
</script>

<!-- Temporary footer
<div class="footer-wip">
  <div class="footer-wip-content">
    This documentation refers to an unreleased version of Netket.
  </div>
</div>
-->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118013987-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-118013987-1');
</script>

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "url": "https://www.netket.org",
  "name": "NetKet",
  "founder": "Giuseppe Carleo",
  "foundingDate": "2018-04-24",
  "foundingLocation" : "New York",
  "logo": "https://www.netket.org/img/logo_small.jpg",
  "sameAs": [
    "https://twitter.com/NetKetOrg",
    "https://github.com/NetKet/netket"
  ],
  "description" : "Netket is an open-source project delivering cutting-edge
  methods for the study of many-body quantum systems with artificial neural
  networks and machine learning techniques."
}
</script>


</body>
</html>